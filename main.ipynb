{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44ef52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd28fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DayToNightDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.day_dir = os.path.join(root_dir, 'day')\n",
    "        self.night_dir = os.path.join(root_dir, 'night')\n",
    "        self.day_images = sorted(os.listdir(self.day_dir))\n",
    "        self.night_images = sorted(os.listdir(self.night_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.day_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        day_path = os.path.join(self.day_dir, self.day_images[idx])\n",
    "        night_path = os.path.join(self.night_dir, self.night_images[idx])\n",
    "        day_img = Image.open(day_path).convert('RGB')\n",
    "        night_img = Image.open(night_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            day_img = self.transform(day_img)\n",
    "            night_img = self.transform(night_img)\n",
    "\n",
    "        return {'day': day_img, 'night': night_img}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebbd5493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Windows\\Temp\\ipykernel_28596\\443411530.py:7: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  dataset = DayToNightDataset(root_dir='D:\\Day to Night Image Conversion using the Pix-Pix GAN\\Dataset', transform=transform)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "dataset = DayToNightDataset(root_dir='D:\\Day to Night Image Conversion using the Pix-Pix GAN\\Dataset', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e485d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pix2Pix GAN: Day to Night Image Translation\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# ------------------\n",
    "# Generator U-Net\n",
    "# ------------------\n",
    "\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, transposed=False, use_dropout=False):\n",
    "        super().__init__()\n",
    "        if not transposed:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        return self.dropout(x) if self.use_dropout else x\n",
    "\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.down1 = nn.Sequential(nn.Conv2d(in_channels, 64, 4, 2, 1), nn.LeakyReLU(0.2, inplace=True))  # No batchnorm\n",
    "        self.down2 = UNetBlock(64, 128)\n",
    "        self.down3 = UNetBlock(128, 256)\n",
    "        self.down4 = UNetBlock(256, 512)\n",
    "        self.down5 = UNetBlock(512, 512)\n",
    "        self.down6 = UNetBlock(512, 512)\n",
    "        self.down7 = UNetBlock(512, 512)\n",
    "        self.down8 = nn.Sequential(nn.Conv2d(512, 512, 4, 2, 1), nn.ReLU(True))  # Bottleneck\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = UNetBlock(512, 512, transposed=True, use_dropout=True)\n",
    "        self.up2 = UNetBlock(1024, 512, transposed=True, use_dropout=True)\n",
    "        self.up3 = UNetBlock(1024, 512, transposed=True, use_dropout=True)\n",
    "        self.up4 = UNetBlock(1024, 512, transposed=True)\n",
    "        self.up5 = UNetBlock(1024, 256, transposed=True)\n",
    "        self.up6 = UNetBlock(512, 128, transposed=True)\n",
    "        self.up7 = UNetBlock(256, 64, transposed=True)\n",
    "\n",
    "        self.final_up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, out_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "\n",
    "        u1 = self.up1(d8)\n",
    "        u2 = self.up2(torch.cat([u1, d7], dim=1))\n",
    "        u3 = self.up3(torch.cat([u2, d6], dim=1))\n",
    "        u4 = self.up4(torch.cat([u3, d5], dim=1))\n",
    "        u5 = self.up5(torch.cat([u4, d4], dim=1))\n",
    "        u6 = self.up6(torch.cat([u5, d3], dim=1))\n",
    "        u7 = self.up7(torch.cat([u6, d2], dim=1))\n",
    "\n",
    "        return self.final_up(torch.cat([u7, d1], dim=1))\n",
    "\n",
    "# ------------------\n",
    "# Discriminator\n",
    "# ------------------\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        if x.size()[2:] != y.size()[2:]:\n",
    "            y = nn.functional.interpolate(y, size=x.size()[2:], mode='bilinear', align_corners=False)\n",
    "        return self.net(torch.cat([x, y], dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9699f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "G = GeneratorUNet().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "criterion_GAN = nn.BCELoss()\n",
    "criterion_L1 = nn.L1Loss()\n",
    "\n",
    "lr = 2e-4\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbf7e2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100] Batch 0 - Loss G: 61.5763 | Loss D: 0.7288\n",
      "[1/100] Batch 0 - Loss G: 24.4277 | Loss D: 0.0659\n",
      "[2/100] Batch 0 - Loss G: 34.4151 | Loss D: 0.6994\n",
      "[3/100] Batch 0 - Loss G: 13.6358 | Loss D: 0.5725\n",
      "[4/100] Batch 0 - Loss G: 23.9366 | Loss D: 0.4086\n",
      "[5/100] Batch 0 - Loss G: 13.4595 | Loss D: 0.7478\n",
      "[6/100] Batch 0 - Loss G: 14.7603 | Loss D: 0.5091\n",
      "[7/100] Batch 0 - Loss G: 12.1266 | Loss D: 0.2881\n",
      "[8/100] Batch 0 - Loss G: 13.6456 | Loss D: 0.6033\n",
      "[9/100] Batch 0 - Loss G: 18.2204 | Loss D: 0.3997\n",
      "[10/100] Batch 0 - Loss G: 9.4045 | Loss D: 0.4152\n",
      "[11/100] Batch 0 - Loss G: 18.9083 | Loss D: 0.5065\n",
      "[12/100] Batch 0 - Loss G: 9.1181 | Loss D: 0.7267\n",
      "[13/100] Batch 0 - Loss G: 10.3714 | Loss D: 0.4170\n",
      "[14/100] Batch 0 - Loss G: 9.8503 | Loss D: 0.5132\n",
      "[15/100] Batch 0 - Loss G: 10.6090 | Loss D: 0.3232\n",
      "[16/100] Batch 0 - Loss G: 5.1191 | Loss D: 0.4533\n",
      "[17/100] Batch 0 - Loss G: 10.4841 | Loss D: 0.6619\n",
      "[18/100] Batch 0 - Loss G: 10.0072 | Loss D: 0.1794\n",
      "[19/100] Batch 0 - Loss G: 9.5811 | Loss D: 0.1900\n",
      "[20/100] Batch 0 - Loss G: 10.7721 | Loss D: 0.0735\n",
      "[21/100] Batch 0 - Loss G: 17.4373 | Loss D: 0.0330\n",
      "[22/100] Batch 0 - Loss G: 8.9412 | Loss D: 0.1227\n",
      "[23/100] Batch 0 - Loss G: 8.2094 | Loss D: 0.0356\n",
      "[24/100] Batch 0 - Loss G: 10.8668 | Loss D: 0.2672\n",
      "[25/100] Batch 0 - Loss G: 8.6796 | Loss D: 0.0810\n",
      "[26/100] Batch 0 - Loss G: 14.8819 | Loss D: 0.0126\n",
      "[27/100] Batch 0 - Loss G: 7.0726 | Loss D: 0.2873\n",
      "[28/100] Batch 0 - Loss G: 9.1635 | Loss D: 0.0235\n",
      "[29/100] Batch 0 - Loss G: 5.5509 | Loss D: 0.9583\n",
      "[30/100] Batch 0 - Loss G: 6.5509 | Loss D: 0.0980\n",
      "[31/100] Batch 0 - Loss G: 10.5768 | Loss D: 0.1359\n",
      "[32/100] Batch 0 - Loss G: 14.0306 | Loss D: 0.0047\n",
      "[33/100] Batch 0 - Loss G: 13.0839 | Loss D: 0.0200\n",
      "[34/100] Batch 0 - Loss G: 7.3798 | Loss D: 0.3310\n",
      "[35/100] Batch 0 - Loss G: 13.0601 | Loss D: 0.1577\n",
      "[36/100] Batch 0 - Loss G: 8.9633 | Loss D: 0.0352\n",
      "[37/100] Batch 0 - Loss G: 8.9218 | Loss D: 0.4236\n",
      "[38/100] Batch 0 - Loss G: 8.4005 | Loss D: 0.0923\n",
      "[39/100] Batch 0 - Loss G: 16.1269 | Loss D: 0.0504\n",
      "[40/100] Batch 0 - Loss G: 8.9625 | Loss D: 0.0891\n",
      "[41/100] Batch 0 - Loss G: 15.1023 | Loss D: 0.0549\n",
      "[42/100] Batch 0 - Loss G: 6.4434 | Loss D: 0.2180\n",
      "[43/100] Batch 0 - Loss G: 6.8135 | Loss D: 0.2721\n",
      "[44/100] Batch 0 - Loss G: 6.3295 | Loss D: 0.5196\n",
      "[45/100] Batch 0 - Loss G: 14.7981 | Loss D: 0.2481\n",
      "[46/100] Batch 0 - Loss G: 3.5330 | Loss D: 0.7092\n",
      "[47/100] Batch 0 - Loss G: 10.3488 | Loss D: 0.0932\n",
      "[48/100] Batch 0 - Loss G: 4.6827 | Loss D: 0.3748\n",
      "[49/100] Batch 0 - Loss G: 7.1218 | Loss D: 0.1154\n",
      "[50/100] Batch 0 - Loss G: 8.9581 | Loss D: 0.7444\n",
      "[51/100] Batch 0 - Loss G: 10.5712 | Loss D: 0.0929\n",
      "[52/100] Batch 0 - Loss G: 10.5540 | Loss D: 0.0198\n",
      "[53/100] Batch 0 - Loss G: 11.7048 | Loss D: 0.0068\n",
      "[54/100] Batch 0 - Loss G: 10.6713 | Loss D: 0.2282\n",
      "[55/100] Batch 0 - Loss G: 11.4263 | Loss D: 0.0474\n",
      "[56/100] Batch 0 - Loss G: 8.7042 | Loss D: 0.4256\n",
      "[57/100] Batch 0 - Loss G: 4.7076 | Loss D: 0.1673\n",
      "[58/100] Batch 0 - Loss G: 12.1862 | Loss D: 0.0298\n",
      "[59/100] Batch 0 - Loss G: 6.6022 | Loss D: 0.4032\n",
      "[60/100] Batch 0 - Loss G: 5.2401 | Loss D: 0.1464\n",
      "[61/100] Batch 0 - Loss G: 8.5590 | Loss D: 0.3099\n",
      "[62/100] Batch 0 - Loss G: 7.8424 | Loss D: 0.0743\n",
      "[63/100] Batch 0 - Loss G: 21.2880 | Loss D: 0.0588\n",
      "[64/100] Batch 0 - Loss G: 9.1554 | Loss D: 0.1979\n",
      "[65/100] Batch 0 - Loss G: 7.6768 | Loss D: 1.1843\n",
      "[66/100] Batch 0 - Loss G: 9.7081 | Loss D: 0.1750\n",
      "[67/100] Batch 0 - Loss G: 8.6843 | Loss D: 0.2151\n",
      "[68/100] Batch 0 - Loss G: 6.5435 | Loss D: 0.2948\n",
      "[69/100] Batch 0 - Loss G: 10.8996 | Loss D: 0.0303\n",
      "[70/100] Batch 0 - Loss G: 20.7937 | Loss D: 0.4103\n",
      "[71/100] Batch 0 - Loss G: 11.3201 | Loss D: 0.0646\n",
      "[72/100] Batch 0 - Loss G: 8.7784 | Loss D: 0.2440\n",
      "[73/100] Batch 0 - Loss G: 5.8705 | Loss D: 0.2825\n",
      "[74/100] Batch 0 - Loss G: 6.3381 | Loss D: 0.3939\n",
      "[75/100] Batch 0 - Loss G: 6.1016 | Loss D: 0.1967\n",
      "[76/100] Batch 0 - Loss G: 17.4036 | Loss D: 0.0591\n",
      "[77/100] Batch 0 - Loss G: 10.5584 | Loss D: 0.0377\n",
      "[78/100] Batch 0 - Loss G: 11.5326 | Loss D: 0.0825\n",
      "[79/100] Batch 0 - Loss G: 7.3209 | Loss D: 0.0806\n",
      "[80/100] Batch 0 - Loss G: 7.8639 | Loss D: 2.1437\n",
      "[81/100] Batch 0 - Loss G: 10.0130 | Loss D: 0.4908\n",
      "[82/100] Batch 0 - Loss G: 9.8223 | Loss D: 0.1496\n",
      "[83/100] Batch 0 - Loss G: 11.9896 | Loss D: 0.0168\n",
      "[84/100] Batch 0 - Loss G: 4.3880 | Loss D: 0.2079\n",
      "[85/100] Batch 0 - Loss G: 6.4091 | Loss D: 0.1109\n",
      "[86/100] Batch 0 - Loss G: 8.2738 | Loss D: 0.1862\n",
      "[87/100] Batch 0 - Loss G: 7.7327 | Loss D: 0.1098\n",
      "[88/100] Batch 0 - Loss G: 5.1785 | Loss D: 0.1451\n",
      "[89/100] Batch 0 - Loss G: 10.8777 | Loss D: 0.0525\n",
      "[90/100] Batch 0 - Loss G: 5.8486 | Loss D: 0.1903\n",
      "[91/100] Batch 0 - Loss G: 6.9774 | Loss D: 0.5182\n",
      "[92/100] Batch 0 - Loss G: 8.1773 | Loss D: 0.1632\n",
      "[93/100] Batch 0 - Loss G: 13.1844 | Loss D: 0.0196\n",
      "[94/100] Batch 0 - Loss G: 5.8761 | Loss D: 0.0648\n",
      "[95/100] Batch 0 - Loss G: 6.1703 | Loss D: 0.0672\n",
      "[96/100] Batch 0 - Loss G: 6.9179 | Loss D: 0.5016\n",
      "[97/100] Batch 0 - Loss G: 11.1799 | Loss D: 0.0067\n",
      "[98/100] Batch 0 - Loss G: 5.8926 | Loss D: 0.2715\n",
      "[99/100] Batch 0 - Loss G: 9.1198 | Loss D: 0.0291\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "epochs = 100\n",
    "lambda_L1 = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        real_A = batch['day'].to(device)\n",
    "        real_B = batch['night'].to(device)\n",
    "        real_labels = torch.ones((real_A.size(0), 1, 30, 30)).to(device)\n",
    "        fake_labels = torch.zeros((real_A.size(0), 1, 30, 30)).to(device)\n",
    "\n",
    "        # Train Generator\n",
    "        fake_B = G(real_A)\n",
    "        D_fake = D(real_A, fake_B)\n",
    "        loss_GAN = criterion_GAN(D_fake, real_labels)\n",
    "        loss_L1 = criterion_L1(fake_B, real_B) * lambda_L1\n",
    "        loss_G = loss_GAN + loss_L1\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Train Discriminator\n",
    "        D_real = D(real_A, real_B)\n",
    "        loss_D_real = criterion_GAN(D_real, real_labels)\n",
    "        D_fake = D(real_A, fake_B.detach())\n",
    "        loss_D_fake = criterion_GAN(D_fake, fake_labels)\n",
    "        loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"[{epoch}/{epochs}] Batch {i} - Loss G: {loss_G.item():.4f} | Loss D: {loss_D.item():.4f}\")\n",
    "            save_image(fake_B * 0.5 + 0.5, f'D:\\\\Day to Night Image Conversion using the Pix-Pix GAN\\\\output/fake_{epoch}_{i}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fa7ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(G.state_dict(), 'pix2pix_generator_day2night.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5565c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load generator\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "G = GeneratorUNet().to(device)\n",
    "G.load_state_dict(torch.load('pix2pix_generator_day2night.pth', map_location=device))\n",
    "G.eval()\n",
    "\n",
    "# Inference function\n",
    "def day_to_night(image):\n",
    "    image = transform(image).unsqueeze(0).to(device)  # [1, 3, 256, 256]\n",
    "    with torch.no_grad():\n",
    "        fake_night = G(image)\n",
    "    fake_night = (fake_night.squeeze().cpu() * 0.5 + 0.5).clamp(0, 1)  # Denormalize to [0, 1]\n",
    "    fake_night_pil = transforms.ToPILImage()(fake_night)\n",
    "    return fake_night_pil\n",
    "\n",
    "# Launch Gradio app\n",
    "gr.Interface(\n",
    "    fn=day_to_night,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Upload a Day Image\"),\n",
    "    outputs=gr.Image(type=\"pil\", label=\"Night Image\"),\n",
    "    title=\"Day to Night Image Converter 🌙\",\n",
    "    description=\"Upload a daytime image and watch it transform into night using a Pix2Pix GAN model!\"\n",
    ").launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
